apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-config
data:
  enabled_plugins: |
    [rabbitmq_management,rabbitmq_peer_discovery_k8s,rabbitmq_shovel,rabbitmq_shovel_management].
  rabbitmq.conf: |
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.k8s.address_type = hostname
    cluster_formation.node_cleanup.interval = 30
    cluster_formation.node_cleanup.only_log_warning = true
    ## https://www.rabbitmq.com/partitions.html
    cluster_partition_handling = ignore
    ## See http://www.rabbitmq.com/ha.html#master-migration-data-locality
    queue_master_locator=min-masters
    ## See http://www.rabbitmq.com/access-control.html#loopback-users
    loopback_users.guest = false
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-startup-scripts
data:
  # This ConfigMap defines a script that is run as the pod starts up.  It exists to create a
  # monitoring user (and any other users we may decide on in the future).  There aren't really
  # any other good options for getting additional users in - the environment variable route only
  # allows for a single user, and if we attempt to import a resources file any users defined
  # there take precedence over the environment variables.
  #
  # By default the monitoring role doesn't have access to any vhosts.  We use this user for
  # Datadog, which access the default vhost via "/api/aliveness-test/%2F".  To enable this to
  # work we need to explicitly grant permissions.  Per
  # https://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2012-November/024132.html these are
  # the minimum necessary permissions.
  users.sh: |
    #!/usr/bin/env bash
    rabbitmqctl wait -t 60 "/var/lib/rabbitmq/mnesia/${RABBITMQ_NODENAME}.pid"
    rabbitmqctl add_user monitoring monitoring
    rabbitmqctl set_user_tags monitoring monitoring
    rabbitmqctl set_permissions -p / monitoring '^aliveness-test$' '^amq\.default$' '^aliveness-test$'
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-probes
data:
  # Define the script used by our readiness probe.
  readiness.sh: |
    #!/usr/bin/env bash
    # This API will return "ok" if there are no active alarms - in that case we're definitely ready
    # for traffic.  However, for our purposes active alarms may be fine, so we need to do further
    # tests if this fails.
    WGET_OUTPUT="$(wget -qO- monitoring:monitoring@localhost:15672/api/healthchecks/node)"
    if [[ "$WGET_OUTPUT" = '{"status":"ok"}' ]]; then
      exit 0
    fi
    # Did this fail due to resource alarms?
    REGEX='^\{"status":"failed","reason":"resource alarm\(s) in effect:\[.*]"}$'
    if [[ "$WGET_OUTPUT" =~ $REGEX ]]; then
      exit 0
    fi
    # If this command failed for some other reason, say we're not ready.
    echo "Unable to determine readiness from '$WGET_OUTPUT'"
    # To investigate such a failure, execute the script manually:
    # kubectl exec --namespace=<NAMESPACE> <POD> -- /probes/readiness.sh
    # You can add debugging output to this script and reapply this file; allow a few minutes for
    # such changes to take effect.
    exit 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-operator-logger-configuration
data:
  # Define additional logging configuration.
  custom-logger-levels.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <Loggers>
        <Root level="info">
            <AppenderRef ref="Console"/>
        </Root>

        <!-- Define additional Loggers here. -->
        <!-- <Logger name="io.fabric8.kubernetes.client" level="trace"/> -->
        <!-- <Logger name="com.indeed.operators.rabbitmq" level="warn"/> -->
    </Loggers>
